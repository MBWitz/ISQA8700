# Import relevant libraries
import pandas as pd
import numpy as np
import patsy as pt
import datetime
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,\
	BaggingClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

### IMPORT DATA, PERFORM PRE-PROCESSING

news = pd.read_csv('C:\\Users\\molly\\Documents\\School\\ISQA8700\\OnlineNewsPopularity.csv')

# Look at columns to gain familiarity with the data
# news.columns

# Dropping data for ease of handling 
news.drop(['url', ' timedelta'], axis = 1, inplace=True)


# eliminate leading space from variable names
news.columns = news.columns.str.strip()


# news.columns

# news.head(10)

pd.to_numeric(news['shares'], errors='coerce')

# print(news.dtypes)

# Add an indicator for shares greater than or equal to 2800 (1) or less than 2800 (0).
def f(row):
    if row['shares'] < 2800:
        val = 0
    else:
        val = 1
    return val

news['gt2800'] = news.apply(f, axis=1)

# news.head(10)

# Obtain number of successes and thier occurance in entire data. 
print(np.sum(news['gt2800']))
print(len(news.index))

### Does not converge when this is done
# take log of 'n_tokens_content' the number of words in each article
# news['n_tokens_log'] = news['n_tokens_content'].apply(np.log)
#for n_tokens_log in news:
#    print(news['n_tokens_log'].unique())
# where_are_NaNs = np.isnan(news['n_tokens_log'])
# news[where_are_NaNs] = 0

# Dropping data that is too highly correlated, or represented by another field
news.drop(['data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_min_max', 'kw_max_max', 'kw_max_avg',
       'self_reference_min_shares', 'self_reference_max_shares', 'weekday_is_monday', 'weekday_is_tuesday',
       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday',
       'weekday_is_sunday','shares','rate_positive_words','rate_negative_words', 'min_positive_polarity',
       'max_positive_polarity','min_negative_polarity','max_negative_polarity','title_subjectivity',
       'title_sentiment_polarity', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_videos',
       'global_sentiment_polarity','global_rate_negative_words', 'n_tokens_title', 'global_rate_positive_words',
       'n_non_stop_words', 'data_channel_is_lifestyle','kw_min_avg','LDA_00','LDA_01','LDA_02','LDA_03','LDA_04'], 
          axis = 1, inplace=True)

# Identify dependent and independent variables:
Y = news['gt2800']
X = news.drop(['gt2800'], axis=1)

# Randomly create train(.67) and test(.33) data sets
x, xt, y, yt = train_test_split(X, Y, test_size = 0.33, random_state=42)

# y based matrices flattened for ease with machine learning codes
y = np.ravel(y)
yt = np.ravel(yt)

# Obtaining descriptive statistics for how many and what percent of each data set is a success
print(np.sum(y))
print(np.sum(yt))
print(100*np.mean(y))
print(100*np.mean(yt))

# Run Least Squares Regression Model
import statsmodels.api as sm

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
model.summary()

# Use Model to predict on test data
xt = sm.add_constant(xt)
predictions = model.predict(xt)

# Look at ROC curve to determine definition of success in predictions
# calculate the fpr and tpr for all thresholds of the classification
from sklearn import metrics

fpr, tpr, threshold = metrics.roc_curve(yt, predictions)
roc_auc = metrics.auc(fpr, tpr)

import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

# Based on ROC curve, test yt cut-off values of .20, .30, .35, and .40
print(((predictions>.35).sum()))

# Sum the actual result with the test prediction for each. If the sum is greater than 1 + cut-off then the prediction 
# of going viral is correct:
yt5 = yt + predictions
print("\nThe correct number of predicts are : %s\n" %str((yt5 >= 1.35).sum()))

# Create a confusion matrix, include the cut-off for definition of success:
metrics.confusion_matrix(yt, predictions>=.35, labels=None, sample_weight=None)
